# -*- coding: utf-8 -*-
"""Credit_Scorepridiction_final_github_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f5hUrdqSeQDzY7tbOEzAe2jVMKTQRuOy
"""

# Import the required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_curve, auc
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, KFold, cross_val_score,GridSearchCV,StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, confusion_matrix, classification_report
from xgboost import plot_importance
import time

#import dataset
loan_data = pd.read_csv('loan_data_2007_2014.csv')
loan_data.shape

# displaying dataset samples without number of column limits

pd.options.display.max_columns = None
loan_data.head()

"""** Data Preprocessing"""

# overview of dataset structure and information
loan_data.info()

#shape of data
loan_data.shape

# plot distribution of loan status feature
plt_1 = plt.figure(figsize=(25,6))
loan_data['loan_status'].value_counts().plot(kind='bar', xlabel='loan_status', ylabel='Frequency', rot=0)

# count of null values in each features
loan_data.isnull().sum()

# identify features with over 80% missing values
na_values = loan_data.isnull().mean()
na_values[na_values>0.8]

# drop features with over 80% missing values
loan_data.dropna(thresh = loan_data.shape[0]*0.2, how = 'all', axis = 1, inplace = True)
loan_data.drop(columns = ['id', 'member_id', 'sub_grade', 'emp_title', 'url', 'desc', 'title', 'zip_code', 'next_pymnt_d',
                          'recoveries', 'collection_recovery_fee', 'total_rec_prncp', 'total_rec_late_fee'], inplace = True)

# overview of dataset structure and information
loan_data.shape, loan_data.info()

# distribution of loan_status values as percentages
loan_data['loan_status'].value_counts(normalize = True)

# categorize loan status into good_bad indicator (add 1 if loan Fully Paid, otherwise 0 for Charged Off', 'Default', 'Late (31-120 days)','Does not meet the credit policy. Status:Charged Off')
loan_data['good_bad'] = np.where(loan_data.loc[:, 'loan_status'].isin(['Charged Off', 'Default', 'Late (31-120 days)',
                                                                       'Does not meet the credit policy. Status:Charged Off']), 0, 1)
# Drop the original 'loan_status' feature
loan_data.drop(columns = ['loan_status'], inplace = True)

# plot distribution of target variable good_bad (1 for Fully Paid, 0 for Not Paid)
loan_data.groupby('good_bad').size().plot(kind='pie', autopct='%.2f')

# separate features and target variable from loan_data
X = loan_data.drop('good_bad', axis = 1)
y = loan_data['good_bad']

# split dataset into 80% training and 20% testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

# create independent copies of training and testing feature sets
X_train, X_test = X_train.copy(), X_test.copy()

"""**Data Cleaning**"""

# Data cleaning - clean up the emp_length feature
def emp_length_converter(df, column):
    df[column] = df[column].str.replace('\+ years', '')
    df[column] = df[column].str.replace('< 1 year', str(0))
    df[column] = df[column].str.replace(' years', '')
    df[column] = df[column].str.replace(' year', '')
    df[column] = pd.to_numeric(df[column])
    df[column].fillna(value = 0, inplace = True)

# apply emp_length_converter() to X_train
emp_length_converter(X_train, 'emp_length')

#convert date columns to datetime format
def date_columns(df, column):
    today_date = pd.to_datetime('2020-08-01')
    df[column] = pd.to_datetime(df[column], format = "%b-%y")
    df['mths_since_' + column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))
    df['mths_since_' + column] = df['mths_since_' + column].apply(lambda x: df['mths_since_' + column].max() if x < 0 else x)
    df.drop(columns = [column], inplace = True)

# apply datetime conversion to date columns in X_train**
date_columns(X_train, 'earliest_cr_line')
date_columns(X_train, 'issue_d')
date_columns(X_train, 'last_pymnt_d')
date_columns(X_train, 'last_credit_pull_d')

# summarize some features in X_train
print(X_train['mths_since_earliest_cr_line'].describe())
print(X_train['mths_since_issue_d'].describe())
print(X_train['mths_since_last_pymnt_d'].describe())
print(X_train['mths_since_last_credit_pull_d'].describe())

# clean and convert 'term' feature to numeric using loan_term_converter()
def loan_term_converter(df, column):
    df[column] = pd.to_numeric(df[column].str.replace(' months', ''))

loan_term_converter(X_train, 'term')

# separate training data into numerical and categorical subsets
X_train_cat = X_train.select_dtypes(include = 'object').copy()
X_train_num = X_train.select_dtypes(include = 'number').copy()

# calculate chi-squared statistic for categorical features
chi2_check = {}

for column in X_train_cat:
    chi, p, dof, ex = chi2_contingency(pd.crosstab(y_train, X_train_cat[column]))
    chi2_check.setdefault('Feature',[]).append(column)
    chi2_check.setdefault('p-value',[]).append(round(p, 10))

# sort chi-squared results by p-value in ascending order
chi2_result = pd.DataFrame(data = chi2_check)
chi2_result.sort_values(by = ['p-value'], ascending = True, ignore_index = True, inplace = True)

# compute ANOVA F-Statistic for numerical features
X_train_num.fillna(X_train_num.mean(), inplace = True)
F_statistic, p_values = f_classif(X_train_num, y_train)
# sorted ANOVA f-statistic results for numerical features
ANOVA_F_table = pd.DataFrame(data = {'Numerical_Feature': X_train_num.columns.values, 'F-Score': F_statistic, 'p values': p_values.round(decimals=10)})
ANOVA_F_table.sort_values(by = ['F-Score'], ascending = False, ignore_index = True, inplace = True)

# extract top 20 numerical features based on anova f-statistic
top_num_features = ANOVA_F_table.iloc[:20,0].to_list()

# visualize pairwise correlations of top 20 numerical features - use heatmap()
corrmat = X_train_num[top_num_features].corr()
plt.figure(figsize=(10,10))
sns.heatmap(corrmat);

# select 4 categorical features with lowest p-values and top 20 numerical features
drop_columns_list = ANOVA_F_table.iloc[20:, 0].to_list()
drop_columns_list.extend(chi2_result.iloc[4:, 0].to_list())
drop_columns_list.extend(['out_prncp_inv', 'total_pymnt_inv'])

#drop specified features from dataset
def col_to_drop(df, columns_list):
    df.drop(columns = columns_list, inplace = True)

# drop specified features from X_train
col_to_drop(X_train, drop_columns_list)

# apply all data cleaning procedures to test data
emp_length_converter(X_test, 'emp_length')
date_columns(X_test, 'earliest_cr_line')
date_columns(X_test, 'issue_d')
date_columns(X_test, 'last_pymnt_d')
date_columns(X_test, 'last_credit_pull_d')
loan_term_converter(X_test, 'term')
col_to_drop(X_test, drop_columns_list)

# reindex test set after dummy variable creation to match training set
X_test = X_test.reindex(labels=X_train.columns, axis=1, fill_value=0)

#exploring X_train and y_train shapes
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
y_train1 = pd.DataFrame(y_train)
y_test1 = pd.DataFrame(y_test)

#dropping columns in X_train and X_test
X_train.drop(columns = ['total_rev_hi_lim','tot_cur_bal',], inplace = True)
X_test.drop(columns = ['total_rev_hi_lim','tot_cur_bal'], inplace = True)

#filling null values with mean
X_train.fillna(X_train.mean(numeric_only=True).round(1), inplace=True)
X_test.fillna(X_test.mean(numeric_only=True).round(1), inplace=True)

X_train.isnull().sum()

X_test.isnull().sum()

#Converting categorical values to numerical on X_train
X_train['grade'] = pd.factorize(X_train['grade'])[0]
X_train['home_ownership'] = pd.factorize(X_train['home_ownership'])[0]
X_train['verification_status'] = pd.factorize(X_train['verification_status'])[0]
X_train['purpose'] = pd.factorize(X_train['purpose'])[0]

#Converting categorical values to numerical on X_test
X_test['grade'] = pd.factorize(X_test['grade'])[0]
X_test['home_ownership'] = pd.factorize(X_test['home_ownership'])[0]
X_test['verification_status'] = pd.factorize(X_test['verification_status'])[0]
X_test['purpose'] = pd.factorize(X_test['purpose'])[0]

#dropping categorical columns: 'purpose', 'home_ownership', 'grade', 'verification_status'
X_train.drop(columns = ['purpose','home_ownership','grade','verification_status'], inplace = True)
X_test.drop(columns = ['purpose','home_ownership','grade','verification_status'], inplace = True)

#data exploration and preparation: X_train and X_test overview
X_train.columns, X_test.columns
print(X_train.shape , X_test.shape, y_train1.shape, y_test1.shape)
y_train2 = y_train1.values[:,-1:]
y_test2 = y_test1.values[:,-1:]
min(y_train2), min(y_test2)

# X_train and X_test overview
X_train2D = X_train
X_test2D = X_test
y_train2D = y_train2
y_test2D = y_test2
X_train2D.shape, X_test2D.shape, y_train2D.shape, y_test2D.shape

#implement Random Forest
start_time = time.time()

#Initialize RandomForestClassifier
rfc = RandomForestClassifier()
#Fit the model with X_train2D and y_train2D
rfc.fit(X_train2D, y_train2D)
#Print the score of the Random Forest Classifier
print("Score of RFC",rfc.score(X_test2D,y_test2D))
#Calculate and print the confusion matrix
print("confusion_matrix :  ", confusion_matrix(y_test2D, rfc.predict(X_test2D)))
#Generate and print the classification report
print(metrics.classification_report(y_test2D,rfc.predict(X_test2D)))
#Perform cross-validation with 10 folds and print mean accuracy with standard deviation
scores = cross_val_score(rfc, X_train2D, y_train2D, cv=10)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
#Print the time taken for execution
print("--- %s seconds ---" % (time.time() - start_time))

#roc curve analysis for random forest classifier
precision, recall, thresholds = roc_curve(y_test2D,rfc.predict(X_test2D))
auc(precision, recall)

#F1 score evaluation for random forest classifier
F1_score = f1_score(y_test2D,rfc.predict(X_test2D), average='macro')
F1_score

#precision and recall evaluation for random forest classifier
precision = precision_score(y_test2D,rfc.predict(X_test2D))
recall = recall_score(y_test2D,rfc.predict(X_test2D))
print(" precision is : ", precision)
print(" recall is : ", recall)

#implement SVM
start_time = time.time()
#Initialize SVM model
SVM_model = SVC()
#Fit the model with X_train and y_train
SVM_model.fit(X_train, y_train)
#Print the score of SVM model
print("Score of RFC",SVM_model.score(X_test,y_test))
#Calculate and print the confusion matrix
print("confusion_matrix :  ", confusion_matrix(y_test, SVM_model.predict(X_test)))
#Generate and print the classification report
print(metrics.classification_report(y_test,SVM_model.predict(X_test)))
#Perform cross-validation with 10 folds and print mean accuracy with standard deviation
scores = cross_val_score(SVM_model, X_train, y_train, cv=10)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
#Print the time taken for execution
print("--- %s seconds ---" % (time.time() - start_time))

# #roc curve analysis for SVM
precision, recall, thresholds = roc_curve(y_test2D,SVM_model.predict(X_test2D))
auc(precision, recall)

# f1_score evaluation for SVM
F1_score = f1_score(y_test2D,SVM_model.predict(X_test2D), average='macro')
F1_score

#precision and recall evaluation for SVM
precision = precision_score(y_test2D,SVM_model.predict(X_test2D))
recall = recall_score(y_test2D,SVM_model.predict(X_test2D))
print(" precision is : ", precision)
print(" recall is : ", recall)

# Commented out IPython magic to ensure Python compatibility.
#importance of the features
# Grid Search
# %matplotlib inline

# model initialization
model = XGBClassifier()
# defining hyperparameters grid
learning_rate = [0.01, 0.1, 0.2]
max_depth=[6]
booster=['gbtree']
n_estimators=[100]
nthread=[6]
min_child_weight=[1,5,10,20,30]

param_grid = dict(min_child_weight=min_child_weight,nthread=nthread,learning_rate=learning_rate,max_depth=max_depth,booster=booster,n_estimators=n_estimators)

# defining cross-validation strategy
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)

# grid search setup
grid_search = GridSearchCV(model, param_grid, scoring="accuracy", n_jobs = 10, cv=kfold)
grid_result = grid_search.fit(X_train2D, y_train2D)

# Summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
print(means, stds, params)

## Evaluate of prediction of model
def evalua(y_pred,y_test):
    # Evaluate of predictions
    accuracy = accuracy_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_pred)
    f1=f1_score(y_test, y_pred)

    # Data test results
    print('Evaluation of predictions: \n')
    print("Accuracy: %.2f%%" % (accuracy * 100.0))
    print("Area ROC: %.2f%%" % (roc * 100.0))
    print("F1 Score: %.2f%%" % (f1 * 100.0))

# Evaluate of predictions
model_best = grid_search.best_estimator_
y_pred2D = model_best.predict(X_test2D)
evalua(y_pred2D,y_test2D)

# roc_curve analysis for XGBOOST
model_best = grid_search.best_estimator_
y_pred2D = model_best.predict(X_test2D)
precision, recall, thresholds = roc_curve(y_test2D,model_best.predict(X_test2D))
print("auc is : ", auc(precision, recall))

#F1_score, precesion and recall evaluation for XGBOOST
F1_score = f1_score(y_test2D,model_best.predict(X_test2D), average='macro')
print("F1_score is :", F1_score)
precision = precision_score(y_test2D,model_best.predict(X_test2D))
recall = recall_score(y_test2D,model_best.predict(X_test2D))
print(" precision is : ", precision)
print(" recall is : ", recall)

#Plotting Feature Importance
plt.figure(figsize=(2880,1440))

ax = plot_importance(model_best)
ax.figure.set_size_inches(12,10)
plt.show()

#Visualizing Confusion Matrix - Test Data
cm2D =confusion_matrix(y_pred2D,y_test2D)
print(cm2D)
plt.clf()
plt.imshow(cm2D, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['Negative','Positive']
plt.title('Confusion Matrix - Test Data')
plt.ylabel('True label')
plt.xlabel('Predicted label')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN','FP'], ['FN', 'TP']]

for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str(cm2D[i][j]))
plt.show()